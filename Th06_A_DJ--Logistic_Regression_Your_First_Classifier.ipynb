{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic Regression - Your First Classifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/schwaaweb/aimlds1_06-TheMachineLearningFramework/blob/master/Th06_A_DJ--Logistic_Regression_Your_First_Classifier.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "6d-s2xCt9GNa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Classification by Logistic Regression\n",
        "\n",
        "We've studied data. We've visualized high dimensional spaces and maximized their variance. We've shaped and reshaped matrixes to get them ready to enter into a model. Today, we're going to classify our first set of data.\n",
        "\n",
        "## Logistic Regression\n",
        "\n",
        "[sklearn.linear_model.LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
        "\n",
        "Logistic regression is a machine learning model that, once trained, estimates the probability that an input value `x` meets some \"requirement\", or doesn't. In logistic regression, `x` is a numerical data-matrix as we are so familiar, and `y` is a label, or class, from a binomial distribution (that is, it is either 0 or 1).\n",
        "\n",
        "\n",
        "## Configuration\n",
        "\n",
        "Maximum likelihood estimation: compute the maximum likelihood function :\n",
        "\n",
        "$p = \\frac{1}{(1 + e^{-(x^tw)})}$\n",
        "\n",
        "for each sample, for each $x_i$:\n",
        "\n",
        "$w_i = w_i + \\alpha(y_i - p)p(1-p)x_i$\n",
        "\n",
        "The first iteration can start with random valies for $w_i$, or they can all begin at $0$.\n",
        "\n",
        "Logistic Regression has at least eight algorithmic solutions, many of which are supported by `sklearn`. Different hyper parameters apply to different algorithms, such as:\n",
        "\n",
        "* learning_rate (alpha)\n",
        "* epochs\n",
        "* starting weights\n",
        "* tolerance for stopping criteria (that is, stop when error < t)\n",
        "\n",
        "### Implementation\n",
        "\n",
        "See [Logistic Regression](http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf)\n",
        "and [A comparison of numerical optimizers for logistic regression](https://tminka.github.io/papers/logreg/minka-logreg.pdf)\n",
        "\n",
        "## Training\n",
        "\n",
        "Before we train a Logistic Regression model, it is important to separate the features and labels into a training and testing set:\n",
        "\n",
        "```\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.33, random_state=42)\n",
        "```\n",
        "\n",
        "Example code:\n",
        "   \n",
        "    regr = linear_model.LogisticRegression()\n",
        "    regr.fit(x_train, y_train)\n",
        "\n",
        "\n",
        "## Classification\n",
        "\n",
        "Models like Logistic Regression are designed to create predictions based on new values:\n",
        "\n",
        "Example code:\n",
        "    \n",
        "    prediction = regr.predict(x)\n",
        "    \n",
        "However, in research practice, models are used to compare their benchmark performance, typically by measuring their test set accuracy.\n",
        "\n",
        "\n",
        "## Testing / Validation\n",
        "\n",
        "For Linear Regression, we \"tested on our training set\", which is a big no-no in machine learning. On higher-powered ML models it is pretty easy to get 100% accuracy when testing on our training set. When a model is perfectly tuned to a single set of data this is called \"overfitting\", and it specifically doesn't tell us about how the model will perform on any previously unseen data.\n",
        "\n",
        "```\n",
        "# Training accuracy:\n",
        "training_accuracy = np.sum(y_hat_train==y_train)/len(y_hat_train)\n",
        "\n",
        "# Test accuracy:\n",
        "y_hat_test = regr.predict(x_test)\n",
        "test_accuracy = np.sum(y_hat_test==y_test)/len(y_hat_test)\n",
        "`````\n",
        "\n",
        "## Measuring results \n",
        "\n",
        "* RMSE\n",
        "\n",
        "Root Mean Squared Error: Relevant for regression of scalar values, not classes. RMSE measures the average error of each sample in the same scale as the samples themselves. When compared with the variance of the data, informs the experimenter of the quality of the model. If $RMSE < \\sigma$, that is, the RMSE is less than the standard deviation of the data, then each prediction is high quality, predicting a value close to what it might actually be. The larger the ratio $\\frac{RMSE}{\\sigma}$, the lower-quality the prediction.\n",
        "\n",
        "* MSE\n",
        "\n",
        "Mean squared error: Related to variance the same way that RMSE is related to $\\sigma$. Use during error calculation because the relative scale of the error is not relevant, only its minimization. This decreases computation time and frequently makes calculating partial derivatives much easier.\n",
        "\n",
        "* SSE\n",
        "\n",
        "Sum of squared-errors: Equal to $n \\times MSE$. SSE is useful in a binary classification problem, as the ratio $\\frac{SSE}{n}$ is the percent error rate. Not useful in the multi-class classification problem\n",
        "\n",
        "$e = \\sum{(\\hat{y}-y)^2}$\n",
        "\n",
        "$accuracy = \\frac{e}{n}$\n",
        "   \n"
      ]
    },
    {
      "metadata": {
        "id": "Zb4JB-FoRoET",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Your Assignment\n",
        "\n",
        "Use `sklearn.linear_model.LogisticRegression` to identify survivors of the titanic dataset. We've already prepared titanic. Now separate it into features `x` and the label `survived = y`. Using LogisticRegression, train a model to predict `y` from `x`.\n",
        "\n",
        "* Try converting the categorical features in titanic into OneHot features. Can you improve results? [One Hot Features](https://yashuseth.blog/2017/12/14/how-to-one-hot-encode-categorical-variables-of-a-large-dataset-in-python/)\n",
        "* Try creating a few \"feature cross\" columns, where you create a new feature in titanic data that is equal to the product of two other features, ie $x_{new} = x_j \\times x_j$ [Feature Crosses](https://developers.google.com/machine-learning/crash-course/feature-crosses/video-lecture)"
      ]
    },
    {
      "metadata": {
        "id": "ggFXuIuwRzYz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Complete assignment here."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FgT8j7qRHpIU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Thinking about your assignment\n",
        "\n",
        "1. What am I being asked to do?\n",
        "2. What coding steps need to be taken to satisfy the problem?\n",
        "3. What must I do to claim that I have \"completed\" the assignment?"
      ]
    }
  ]
}